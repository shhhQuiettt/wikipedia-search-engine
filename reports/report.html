<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Wikipedia search engine</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="reports/styling.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Wikipedia search engine</h1>
</header>
<h1 id="wikipedia-search-engine-and-recommender-system">Wikipedia Search
Engine and Recommender System</h1>
<h2 id="link-to-github">(<a
href="https://github.com/shhhQuiettt/wikipedia-search-engine">Link to
Github</a>)</h2>
<p>The aim of this project is to index the Wikipedia and build a
recommender system in simplified form on top of it. Given a list of
previously read articles, the system should recommend the next article
to read. In this simplified form we assume that the documents are
related, for example that they come from a <strong>single
session</strong>, and we want to recommend the next article(s) to
read.</p>
<p>Process consists of following steps:</p>
<ol type="1">
<li><strong>Building the database</strong>
<ol type="1">
<li>Downloading the Wikipedia dump</li>
<li>Text Preprocessing</li>
<li>Indexing</li>
<li>Storing the index</li>
</ol></li>
<li><strong>Recommending</strong>
<ol type="1">
<li>Querying the read documents</li>
<li>Calculating similarities</li>
<li>Recommending the k-next articles</li>
</ol></li>
</ol>
<h2 id="usage">Usage</h2>
<ol type="1">
<li><strong>Building the database</strong> Run</li>
</ol>
<pre><code>python build_db.py [INITIAL_URL]</code></pre>
<p>to start the crawling process with an <em>optional</em> initial
url.</p>
<ol start="2" type="1">
<li><strong>Recommending</strong> Put visited links inside
<code>previously_seen.txt</code> and run:</li>
</ol>
<pre><code>python recommend.py</code></pre>
<p>to get the recommended articles.</p>
<h1 id="implementation">Implementation</h1>
<h2 id="building-the-database">Building the database</h2>
<p>The building and indexing is done <strong>asynchronously</strong>.
The crawling process is implemented in a coroutine model on a one thread
(<code>crawling.py</code>), while the indexing is done in a separate
thread with multiple workers (<code>indexing.py</code>).</p>
<h3 id="downloading-the-wikipedia-dump">Downloading the Wikipedia
dump</h3>
<p>The crawler start from <strong>three initial urls</strong> to: - An
<em>important</em> mathematical theorem: <a
href="https://en.wikipedia.org/wiki/Hairy_ball_theorem">Hairy Ball
Theorem</a> - Butterjelly sandwich: <a
href="https://en.wikipedia.org/wiki/Butterjelly_sandwich">Butterjelly
sandwich</a> - Hedgehog: <a
href="https://en.wikipedia.org/wiki/Hedgehog">Hedgehog</a></p>
<p>Then <code>BeautifulSoup</code> is used to parse the html and extract
the text from the body of the document. We also extract the title of the
document. The document gets inside a queue, where it will await <a
href="">preprocessing and indexing</a>, and we search for all urls in
the body to apply the procedure recursively.</p>
<h3 id="text-preprocessing">Text Preprocessing</h3>
<p>The text is preprocessed in the following way: 1. Tokenization We
tokenize the text using <code>nltk</code>’s <code>word_tokenize</code>
function to obtain a list of tokens.</p>
<ol start="2" type="1">
<li>Lemmatization We lemmatize the tokens using <code>nltk</code>’s
<code>WordNetLemmatizer</code> to extract the base word of the token.
For example the word “running” will be lemmatized to “run”.</li>
</ol>
<h3 id="indexing">Indexing</h3>
<p>We want to store <strong>TF-IDF</strong> values for each word in
document:</p>
<p><span class="math display">\[ \text{TF}(\text{term}, \text{document})
= \frac{\#\text{ term appears in document}}{\#\text{ the most frequent
term in document}} \]</span></p>
<p><span class="math display">\[ \text{IDF}(\text{term}) = \log \left(
\frac{\text{total number of documents}}{\text{number of documents
containing term}} \right) \]</span></p>
<p><span class="math display">\[ \text{TF-IDF}(\text{term},
\text{document}) = \text{TF}(\text{term}, \text{document}) \times
\text{IDF}(\text{term}) \]</span></p>
<p>The <strong>IDF</strong> is important in order to prioritize terms
with <strong>high entropy</strong> among other documents</p>
<h3 id="storing-the-index">Storing the index</h3>
<p>In the current implementation we store inverted index inside
<strong>sqlite</strong> database, but we abstract the index storage to
an <em>abstract class</em> <code>InvertedIndex</code> to possibly test
other storage methods like <strong>NoSQL</strong> databases like
<em>mongoDB</em> or <strong>RAM-base</strong> like <em>Redis</em></p>
<h2 id="recommending">Recommending</h2>
<p>The read documents are queried from the database and the similarities
are calculated using the <strong>cosine similarity</strong>. The cosine
similarity is calculated as follows:</p>
<p><span class="math display">\[ \text{cosine similarity}(\text{doc1},
\text{doc2}) = \frac{\text{doc1} \cdot \text{doc2}}{\| \text{doc1} \|
\times \| \text{doc2} \|} \]</span></p>
<p>Where document vector is a vector of <strong>TF-IDF</strong> values
for each word in the document.</p>
<p>The implementation allows for using other similarity measures which
fulfill the <code>similarity_function</code> interface
(<code>recommender.py</code>), so we also tested <strong>Jaccard
similarity</strong> with binary vectors:</p>
<p><span class="math display">\[ \text{Jaccard similarity}(\text{doc1},
\text{doc2}) = \frac{\text{doc1} \cap \text{doc2}}{\text{doc1} \cup
\text{doc2}} \]</span></p>
<p>In the currenct implementation we assume that the documents come from
a <strong>single session</strong> and we want to recommend the next
<code>k</code> article(s) to read.</p>
<p>We calculate the <strong>centroid</strong> of the read documents and
recommend the <code>k</code> closest documents to the centroid.</p>
<h3 id="example-results">Example results</h3>
<p>Running the recommender on the following documents from
<code>./example_visited/previously_seen1.txt</code> with mathematical
researchers</p>
<ul>
<li>Leonhard Euler</li>
<li>Isaac Newton</li>
<li>Mathematics</li>
<li>Functions</li>
<li>Real number</li>
</ul>
<p>We get the following recommendations:</p>
<ol type="1">
<li>Calculus:</li>
</ol>
<ul>
<li>Cosine similarity: 0.06</li>
</ul>
<ol start="2" type="1">
<li>Euclidean space:</li>
</ol>
<ul>
<li>Cosine similarity: 0.058</li>
</ul>
<ol start="3" type="1">
<li>List of important publications in mathematics:</li>
</ol>
<ul>
<li>Cosine similarity: 0.056</li>
</ul>
<ol start="4" type="1">
<li>Category theory:</li>
</ol>
<ul>
<li>Cosine similarity: 0.049</li>
</ul>
<ol start="5" type="1">
<li>Manifold:</li>
</ol>
<ul>
<li>Cosine similarity: 0.045</li>
</ul>
<p>Which seems reasonable, given that the database contains a lot of
non-mathematical articles related to butterjelly sandwiches and
hedgehogs.</p>
</body>
</html>
