<h2 id="link-to-github">(<a href="https://github.com/shhhQuiettt/wikipedia-search-engine">Link to
Github</a>)</h2>
<p>The aim of this project is to index the Wikipedia and build a
<strong>recommender system</strong> in a simplified form on top of it.
Given a list of previously read articles, the system should recommend
the next article to read.</p>
<p>In this simplified form we assume that the documents are related, for
example that they come from a <strong>single session</strong>, and we
want to recommend the next article(s) to read.</p>
<h3 id="process-consists-of-following-steps">Process consists of
following steps:</h3>
<ol type="1">
<li><p><strong>Building the database</strong></p>
<ol type="1">
<li>Downloading the Wikipedia dump</li>
<li>Text Preprocessing</li>
<li>Indexing</li>
<li>Storing the index</li>
</ol></li>
<li><p><strong>Recommending</strong></p>
<ol type="1">
<li>Querying the read documents</li>
<li>Calculating similarities</li>
<li>Recommending the k-next articles</li>
</ol></li>
</ol>
<h2 id="usage">Usage</h2>
<ol type="1">
<li><strong>Building the database</strong></li>
</ol>
<p>Run this command to start the crawling and database building
process:</p>
<pre><code>python build_db.py</code></pre>
<ol start="2" type="1">
<li><strong>Recommending</strong></li>
</ol>
<p>Put visited links inside <code>previously_seen.txt</code> and run
this command to get 5 recommendations:</p>
<pre><code>python recommend.py 5 previously_seen.txt</code></pre>
<h1 id="implementation">Implementation</h1>
<h2 id="building-the-database">Building the database</h2>
<p>The building and indexing is done <strong>asynchronously</strong>.
The crawling process is implemented in a coroutine model on a one thread
(<a href="https://github.com/shhhQuiettt/wikipedia-search-engine/blob/main/crawler.py"><code>crawler.py</code></a>),
while the indexing is done in a separate thread with multiple workers
(<a href="https://github.com/shhhQuiettt/wikipedia-search-engine/blob/main/indexing.py"><code>indexing.py</code></a>).</p>
<h3 id="downloading-the-wikipedia-dump">Downloading the Wikipedia
dump</h3>
<p>The crawler start from <strong>three initial urls</strong> to:</p>
<ul>
<li>An <em>important</em> mathematical theorem: <a href="https://en.wikipedia.org/wiki/Hairy_ball_theorem">Hairy Ball
Theorem</a></li>
<li>Butterjelly sandwich: <a href="https://en.wikipedia.org/wiki/Butterjelly_sandwich">Butterjelly
sandwich</a></li>
<li>Hedgehog: <a href="https://en.wikipedia.org/wiki/Hedgehog">Hedgehog</a></li>
</ul>
<p>Then <code>BeautifulSoup</code> is used to parse the html and extract
the text from the body of the document. We also extract the title of the
document. The document gets inside a queue, where it will await
<strong>preprocessing and indexing</strong>, and we search for all urls
in the body to apply the procedure recursively.</p>
<h3 id="text-preprocessing">Text Preprocessing</h3>
<p>The text is preprocessed in the following way:</p>
<ol type="1">
<li><strong>Tokenization</strong></li>
</ol>
<p>We tokenize the text using <code>nltk</code>’s
<code>word_tokenize</code> function to obtain a list of tokens.</p>
<ol start="2" type="1">
<li><strong>Lemmatization</strong></li>
</ol>
<p>We lemmatize the tokens using <code>nltk</code>’s
<code>WordNetLemmatizer</code> to extract the base word of the token.
For example the word “running” will be lemmatized to “run”.</p>
<ol start="3" type="1">
<li><strong>Stopwords removal</strong></li>
</ol>
<p>We remove the stopwords using <code>nltk</code>’s
<code>stopwords</code> list.</p>
<h3 id="indexing">Indexing</h3>
<p>We want to store <strong>TF-IDF</strong> values for each word in
document:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">TF</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">term</mtext><mo>,</mo><mtext mathvariant="normal">document</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mtext mathvariant="normal"># term appears in document</mtext><mtext mathvariant="normal"># the most frequent term in document</mtext></mfrac></mrow><annotation encoding="application/x-tex"> \text{TF}(\text{term}, \text{document}) = \frac{\text{# term appears in document}}{\text{# the most frequent term in document}} </annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">IDF</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">term</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>log</mo><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mtext mathvariant="normal">total number of documents</mtext><mtext mathvariant="normal">number of documents containing term</mtext></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \text{IDF}(\text{term}) = \log \left( \frac{\text{total number of documents}}{\text{number of documents containing term}} \right) </annotation></semantics></math></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">TF-IDF</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">term</mtext><mo>,</mo><mtext mathvariant="normal">document</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mtext mathvariant="normal">TF</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">term</mtext><mo>,</mo><mtext mathvariant="normal">document</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>×</mo><mtext mathvariant="normal">IDF</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">term</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex"> \text{TF-IDF}(\text{term}, \text{document}) = \text{TF}(\text{term}, \text{document}) \times \text{IDF}(\text{term}) </annotation></semantics></math></p>
<p>The <strong>IDF</strong> is important in order to prioritize terms
with <strong>high entropy</strong> among other documents</p>
<h3 id="storing-the-index">Storing the index</h3>
<p>In the current implementation we store inverted index inside
<strong>sqlite</strong> database, but we abstract the index storage to
an <em>abstract class</em> <code>InvertedIndex</code> to possibly test
other storage methods like <strong>NoSQL</strong> databases like
<em>mongoDB</em> or <strong>RAM-base</strong> like <em>Redis</em></p>
<h1 id="recommending">Recommending</h1>
<p>The read documents are queried from the database and the similarities
are calculated using the <strong>cosine similarity</strong>. The cosine
similarity is calculated as follows:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">cosine similarity</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">doc1</mtext><mo>,</mo><mtext mathvariant="normal">doc2</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mtext mathvariant="normal">doc1</mtext><mo>⋅</mo><mtext mathvariant="normal">doc2</mtext></mrow><mrow><mo stretchy="false" form="postfix">∥</mo><mtext mathvariant="normal">doc1</mtext><mo stretchy="false" form="postfix">∥</mo><mo>×</mo><mo stretchy="false" form="postfix">∥</mo><mtext mathvariant="normal">doc2</mtext><mo stretchy="false" form="postfix">∥</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \text{cosine similarity}(\text{doc1}, \text{doc2}) = \frac{\text{doc1} \cdot \text{doc2}}{\| \text{doc1} \| \times \| \text{doc2} \|} </annotation></semantics></math></p>
<p>Where document vector is a vector of <strong>TF-IDF</strong> values
for each word in the document.</p>
<p>The implementation allows for using other similarity measures which
fulfill the <code>similarity_function</code> interface
(<code>recommender.py</code>), so we also tested <strong>Jaccard
similarity</strong> with binary vectors:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Jaccard similarity</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">doc1</mtext><mo>,</mo><mtext mathvariant="normal">doc2</mtext><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mtext mathvariant="normal">doc1</mtext><mo>∩</mo><mtext mathvariant="normal">doc2</mtext></mrow><mrow><mtext mathvariant="normal">doc1</mtext><mo>∪</mo><mtext mathvariant="normal">doc2</mtext></mrow></mfrac></mrow><annotation encoding="application/x-tex"> \text{Jaccard similarity}(\text{doc1}, \text{doc2}) = \frac{\text{doc1} \cap \text{doc2}}{\text{doc1} \cup \text{doc2}} </annotation></semantics></math></p>
<p>In the currenct implementation we assume that the documents come from
a <strong>single session</strong> and we want to recommend the next
<code>k</code> article(s) to read.</p>
<p>We calculate the <strong>centroid</strong> of the read documents and
recommend the <code>k</code> closest documents to the centroid.</p>
