<h2 id="link-to-github">(<a href="https://github.com/shhhQuiettt/wikipedia-search-engine">Link to
Github</a>)</h2>
<p>The aim of this project is to index the Wikipedia and build a
<strong>recommender system</strong> in a simplified form on top of it.
Given a list of previously read articles, the system should recommend
the next article to read.</p>
<p>In this simplified form we assume that the documents are related, for
example that they come from a <strong>single session</strong>, and we
want to recommend the next article(s) to read.</p>
<h3 id="process-consists-of-following-steps">Process consists of
following steps:</h3>
<ol type="1">
<li><p><strong>Building the database</strong></p>
<ol type="1">
<li>Downloading the Wikipedia dump</li>
<li>Text Preprocessing</li>
<li>Indexing</li>
<li>Storing the index</li>
</ol></li>
<li><p><strong>Recommending</strong></p>
<ol type="1">
<li>Querying the read documents</li>
<li>Calculating similarities</li>
<li>Recommending the k-next articles</li>
</ol></li>
</ol>
<h2 id="usage">Usage</h2>
<ol type="1">
<li><strong>Building the database</strong></li>
</ol>
<p>Run this command to start the crawling and database building
process:</p>
<pre><code>python build_db.py</code></pre>
<ol start="2" type="1">
<li><strong>Recommending</strong></li>
</ol>
<p>Put visited links inside <code>previously_seen.txt</code> and run
this command to get 5 recommendations:</p>
<pre><code>python recommend.py 5 previously_seen.txt</code></pre>
<h1 id="implementation">Implementation</h1>
<h2 id="building-the-database">Building the database</h2>
<p>The building and indexing is done <strong>asynchronously</strong>.
The crawling process is implemented in a coroutine model on a one thread
(<a href="https://github.com/shhhQuiettt/wikipedia-search-engine/blob/main/crawler.py"><code>crawler.py</code></a>),
while the indexing is done in a separate thread with multiple workers
(<a href="https://github.com/shhhQuiettt/wikipedia-search-engine/blob/main/indexing.py"><code>indexing.py</code></a>).</p>
<h3 id="downloading-the-wikipedia-dump">Downloading the Wikipedia
dump</h3>
<p>The crawler start from <strong>three initial urls</strong> to:</p>
<ul>
<li>An <em>important</em> mathematical theorem: <a href="https://en.wikipedia.org/wiki/Hairy_ball_theorem">Hairy Ball
Theorem</a></li>
<li>Butterjelly sandwich: <a href="https://en.wikipedia.org/wiki/Butterjelly_sandwich">Butterjelly
sandwich</a></li>
<li>Hedgehog: <a href="https://en.wikipedia.org/wiki/Hedgehog">Hedgehog</a></li>
</ul>
<p>Then <code>BeautifulSoup</code> is used to parse the html and extract
the text from the body of the document. We also extract the title of the
document. The document gets inside a queue, where it will await
<strong>preprocessing and indexing</strong>, and we search for all urls
in the body to apply the procedure recursively.</p>
<h3 id="text-preprocessing">Text Preprocessing</h3>
<p>The text is preprocessed in the following way:</p>
<ol type="1">
<li><strong>Tokenization</strong></li>
</ol>
<p>We tokenize the text using <code>nltk</code>’s
<code>word_tokenize</code> function to obtain a list of tokens.</p>
<ol start="2" type="1">
<li><strong>Lemmatization</strong></li>
</ol>
<p>We lemmatize the tokens using <code>nltk</code>’s
<code>WordNetLemmatizer</code> to extract the base word of the token.
For example the word “running” will be lemmatized to “run”.</p>
<ol start="3" type="1">
<li><strong>Stopwords removal</strong></li>
</ol>
<p>We remove the stopwords using <code>nltk</code>’s
<code>stopwords</code> list.</p>
<h3 id="indexing">Indexing</h3>
<p>We want to store <strong>TF-IDF</strong> values for each word in
document:</p>
<p><span class="math display">\[ \text{TF}(\text{term}, \text{document})
= \frac{\#\text{ term appears in document}}{\#\text{ the most frequent
term in document}} \]</span></p>
<p><span class="math display">\[ \text{IDF}(\text{term}) = \log \left(
\frac{\text{total number of documents}}{\text{number of documents
containing term}} \right) \]</span></p>
<p><span class="math display">\[ \text{TF-IDF}(\text{term},
\text{document}) = \text{TF}(\text{term}, \text{document}) \times
\text{IDF}(\text{term}) \]</span></p>
<p>The <strong>IDF</strong> is important in order to prioritize terms
with <strong>high entropy</strong> among other documents</p>
<h3 id="storing-the-index">Storing the index</h3>
<p>In the current implementation we store inverted index inside
<strong>sqlite</strong> database, but we abstract the index storage to
an <em>abstract class</em> <code>InvertedIndex</code> to possibly test
other storage methods like <strong>NoSQL</strong> databases like
<em>mongoDB</em> or <strong>RAM-base</strong> like <em>Redis</em></p>
<h1 id="recommending">Recommending</h1>
<p>The read documents are queried from the database and the similarities
are calculated using the <strong>cosine similarity</strong>. The cosine
similarity is calculated as follows:</p>
<p><span class="math display">\[ \text{cosine similarity}(\text{doc1},
\text{doc2}) = \frac{\text{doc1} \cdot \text{doc2}}{\| \text{doc1} \|
\times \| \text{doc2} \|} \]</span></p>
<p>Where document vector is a vector of <strong>TF-IDF</strong> values
for each word in the document.</p>
<p>The implementation allows for using other similarity measures which
fulfill the <code>similarity_function</code> interface
(<code>recommender.py</code>), so we also tested <strong>Jaccard
similarity</strong> with binary vectors:</p>
<p><span class="math display">\[ \text{Jaccard similarity}(\text{doc1},
\text{doc2}) = \frac{\text{doc1} \cap \text{doc2}}{\text{doc1} \cup
\text{doc2}} \]</span></p>
<p>In the currenct implementation we assume that the documents come from
a <strong>single session</strong> and we want to recommend the next
<code>k</code> article(s) to read.</p>
<p>We calculate the <strong>centroid</strong> of the read documents and
recommend the <code>k</code> closest documents to the centroid.</p>
<h3 id="example-results">Example results</h3>
<p>Running the recommender on the following documents from
<code>./example_visited/previously_seen1.txt</code> with mathematical
researchers</p>
<ul>
<li>Leonhard Euler</li>
<li>Isaac Newton</li>
<li>Mathematics</li>
<li>Functions</li>
<li>Real number</li>
</ul>
<p>We get the following recommendations:</p>
<ol type="1">
<li>Calculus:</li>
</ol>
<ul>
<li>Cosine similarity: 0.06</li>
</ul>
<ol start="2" type="1">
<li>Euclidean space:</li>
</ol>
<ul>
<li>Cosine similarity: 0.058</li>
</ul>
<ol start="3" type="1">
<li>List of important publications in mathematics:</li>
</ol>
<ul>
<li>Cosine similarity: 0.056</li>
</ul>
<ol start="4" type="1">
<li>Category theory:</li>
</ol>
<ul>
<li>Cosine similarity: 0.049</li>
</ul>
<ol start="5" type="1">
<li>Manifold:</li>
</ol>
<ul>
<li>Cosine similarity: 0.045</li>
</ul>
<p>Which seems reasonable, given that the database contains a lot of
non-mathematical articles related to butterjelly sandwiches and
hedgehogs.</p>
